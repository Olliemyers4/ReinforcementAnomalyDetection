{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import easydict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/JoungheeKim/autoencoder-lstm/blob/main/autoencoderLSTM_tutorial(english).ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,inputSize=3,hiddenSize=64,layers=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden = hiddenSize\n",
    "        self.layers = layers\n",
    "        self.lstm = nn.LSTM(inputSize, hiddenSize, layers, batch_first=True, dropout=0.1, bidirectional=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output, (hidden, cell) = self.lstm(x)\n",
    "        return (hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,inputSize=3,hiddenSize=64,layers=2,outputSize=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden = hiddenSize\n",
    "        self.layers = layers\n",
    "        self.lstm = nn.LSTM(inputSize, hiddenSize, layers, batch_first=True, dropout=0.1, bidirectional=False)\n",
    "        self.fc = nn.Linear(hiddenSize, outputSize)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, (hidden, cell) = self.lstm(x, hidden)\n",
    "        prediction = self.fc(output)\n",
    "        return prediction, (hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(Autoencoder,self).__init__()\n",
    "        self.encoder = Encoder(args.inputSize, args.hiddenSize, args.layers)\n",
    "        self.decoder = Decoder(args.inputSize, args.hiddenSize, args.layers, args.inputSize) # inputSize = outputSize - as you want to reconstruct the input\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.layers = args.layers\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size = x.size(0)\n",
    "        if hidden is None:\n",
    "            _, (hidden, cell) = self.encoder(x)\n",
    "            hidden = hidden.repeat(self.layers, 1, 1)\n",
    "            cell = cell.repeat(self.layers, 1, 1)\n",
    "        else:\n",
    "            hidden, cell = hidden\n",
    "\n",
    "        decode, _ = self.decoder(x, (hidden, cell))\n",
    "        return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Autoencoder(inputSize=3, hiddenSize=64, layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args,model,train,test):\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=args.learningRate)\n",
    "\n",
    "    epochs = range(args.maxIter//len(train)+1) #fix var name of max_iter\n",
    "\n",
    "    count = 0\n",
    "    for epoch in epochs:\n",
    "        model.train()\n",
    "        optim.zero_grad()\n",
    "        for i, data in enumerate(train):\n",
    "            if count > args.maxIter:\n",
    "                return model\n",
    "            count += 1\n",
    "\n",
    "            input_data = data.unsqueeze(1).to(args.device)\n",
    "            #print(\"Shape of input_data:\", input_data.shape)  # Add this line to print the shape of input_data\n",
    "\n",
    "            # Initialize hidden and cell states\n",
    "            batch_size = input_data.size(0)\n",
    "            hidden = torch.zeros((args.layers * 1, batch_size, args.hiddenSize)).to(args.device)\n",
    "            cell = torch.zeros((args.layers * 1, batch_size, args.hiddenSize)).to(args.device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input_data, (hidden, cell))\n",
    "            loss = model.criterion(output, input_data)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        model.eval()\n",
    "        evalLoss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test):\n",
    "                input_data = data.unsqueeze(1).to(args.device)\n",
    "                # Initialize hidden and cell states\n",
    "                batch_size = input_data.size(0)\n",
    "                hidden = torch.zeros((args.layers * 1, batch_size, args.hiddenSize)).to(args.device)\n",
    "                cell = torch.zeros((args.layers * 1, batch_size, args.hiddenSize)).to(args.device)\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(input_data, (hidden, cell))\n",
    "                loss = model.criterion(output, input_data)\n",
    "                evalLoss += loss.item()\n",
    "        evalLoss /= len(test)\n",
    "        print(f\"Epoch {epoch} - Eval Loss: {evalLoss}\")\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## option Setting\n",
    "args = easydict.EasyDict({\n",
    "    \"batch_size\": 64, ## batch size setting\n",
    "    \"device\": torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'), ## whether use GPU\n",
    "    \"inputSize\": 3, ## input dimension setting (image is 64x64 = 4096)\n",
    "    \"hiddenSize\": 64, ## Hidden dimension setting\n",
    "    \"outputSize\": 3, ## output dimension setting\n",
    "    \"layers\": 2,     ## number of LSTM layer\n",
    "    \"learningRate\" : 0.0005, ## learning rate setting\n",
    "    \"maxIter\" : 100, ## max iteration setting\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Step 1: Prepare the Dataset\n",
    "clear = pd.read_csv('testClear.csv')\n",
    "# Assuming your data is in a single column and each row represents a timestep\n",
    "timeseries_tensorC = torch.tensor(clear[['value-0','value-1','value-2']].values, dtype=torch.float32)\n",
    "\n",
    "anom = pd.read_csv('testAnom.csv')\n",
    "# Assuming your data is in a single column and each row represents a timestep\n",
    "timeseries_tensorA = torch.tensor(anom[['value-0','value-1','value-2']].values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Create a Custom Dataset Class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Step 3: Use DataLoader to Load the Data\n",
    "datasetC = TimeSeriesDataset(timeseries_tensorC)\n",
    "datasetA = TimeSeriesDataset(timeseries_tensorA)\n",
    "batch_size = 64  # Set your desired batch size\n",
    "shuffle = False  # Set to True to have the data reshuffled at every epoch\n",
    "\n",
    "dataLoaderC = DataLoader(datasetC, batch_size=batch_size, shuffle=shuffle)\n",
    "dataLoaderA = DataLoader(datasetA, batch_size=1, shuffle=shuffle)\n",
    "\n",
    "# Usage example:\n",
    "# Iterate through the data_loader to get batches of data\n",
    "#for batch in data_loader:\n",
    "#    # Process each batch as needed\n",
    "#    print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Encoder(\n",
       "    (lstm): LSTM(3, 64, num_layers=2, batch_first=True, dropout=0.1)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (lstm): LSTM(3, 64, num_layers=2, batch_first=True, dropout=0.1)\n",
       "    (fc): Linear(in_features=64, out_features=3, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (criterion): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Autoencoder(args)\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Eval Loss: 0.5631278268992901\n",
      "Epoch 1 - Eval Loss: 0.4964159242808819\n",
      "Epoch 2 - Eval Loss: 0.4126966018229723\n",
      "Epoch 3 - Eval Loss: 0.3010512348264456\n",
      "Epoch 4 - Eval Loss: 0.1704719951376319\n",
      "Epoch 5 - Eval Loss: 0.07940431265160441\n"
     ]
    }
   ],
   "source": [
    "model = train(args,model,dataLoaderC,dataLoaderC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Loss: 0.12021169066429138\n",
      "1: Loss: 0.011330842040479183\n",
      "2: Loss: 0.14898079633712769\n",
      "3: Loss: 0.055175043642520905\n",
      "4: Loss: 0.014610067009925842\n",
      "5: Loss: 0.15678837895393372\n",
      "6: Loss: 0.01095215231180191\n",
      "7: Loss: 0.07301009446382523\n",
      "8: Loss: 0.128949373960495\n",
      "9: Loss: 0.018408160656690598\n",
      "10: Loss: 0.10594461858272552\n",
      "11: Loss: 0.02278931252658367\n",
      "12: Loss: 0.132229745388031\n",
      "13: Loss: 0.049364231526851654\n",
      "14: Loss: 0.011471264064311981\n",
      "15: Loss: 0.12452933192253113\n",
      "16: Loss: 0.005029718391597271\n",
      "17: Loss: 0.0516853928565979\n",
      "18: Loss: 0.08461159467697144\n",
      "19: Loss: 0.02872471511363983\n",
      "20: Loss: 0.08099289983510971\n",
      "21: Loss: 0.026906277984380722\n",
      "22: Loss: 0.12402968108654022\n",
      "23: Loss: 0.05757363885641098\n",
      "24: Loss: 0.021062541753053665\n",
      "25: Loss: 0.15346187353134155\n",
      "26: Loss: 0.02291017584502697\n",
      "27: Loss: 0.06240483373403549\n",
      "28: Loss: 0.11845191568136215\n",
      "29: Loss: 0.030521808192133904\n",
      "30: Loss: 0.11062854528427124\n",
      "31: Loss: 0.02420293539762497\n",
      "32: Loss: 0.12557797133922577\n",
      "33: Loss: 0.05798869580030441\n",
      "34: Loss: 0.02592676877975464\n",
      "35: Loss: 0.11897090077400208\n",
      "36: Loss: 0.01114979013800621\n",
      "37: Loss: 0.05378856509923935\n",
      "38: Loss: 0.09931179881095886\n",
      "39: Loss: 0.024295024573802948\n",
      "40: Loss: 0.12146154046058655\n",
      "41: Loss: 0.029788976535201073\n",
      "42: Loss: 0.11765548586845398\n",
      "43: Loss: 0.04545100778341293\n",
      "44: Loss: 0.014868672005832195\n",
      "45: Loss: 0.10672010481357574\n",
      "46: Loss: 0.011435157619416714\n",
      "47: Loss: 0.060914717614650726\n",
      "48: Loss: 0.15865397453308105\n",
      "49: Loss: 0.030528437346220016\n",
      "50: Loss: 0.09995629638433456\n",
      "51: Loss: 0.051830217242240906\n",
      "52: Loss: 0.10349936783313751\n",
      "53: Loss: 0.04683729261159897\n",
      "54: Loss: 0.009962530806660652\n",
      "55: Loss: 0.12103547900915146\n",
      "56: Loss: 0.006436729338020086\n",
      "57: Loss: 0.0543864369392395\n",
      "58: Loss: 0.11630038917064667\n",
      "59: Loss: 0.012663784436881542\n",
      "60: Loss: 0.11722439527511597\n",
      "61: Loss: 0.045555856078863144\n",
      "62: Loss: 0.13344675302505493\n",
      "63: Loss: 0.05132663995027542\n",
      "64: Loss: 0.011962808668613434\n",
      "65: Loss: 0.10921627283096313\n",
      "66: Loss: 0.01041345577687025\n",
      "67: Loss: 0.07045992463827133\n",
      "68: Loss: 0.1297564059495926\n",
      "69: Loss: 0.019222978502511978\n",
      "70: Loss: 0.09907901287078857\n",
      "71: Loss: 0.027665648609399796\n",
      "72: Loss: 0.11123021692037582\n",
      "73: Loss: 0.06245748698711395\n",
      "74: Loss: 0.007230055518448353\n",
      "75: Loss: 0.13773581385612488\n",
      "76: Loss: 0.02485470473766327\n",
      "77: Loss: 0.05824990198016167\n",
      "78: Loss: 0.13229438662528992\n",
      "79: Loss: 0.035341423004865646\n",
      "80: Loss: 0.10695250332355499\n",
      "81: Loss: 0.029970020055770874\n",
      "82: Loss: 0.13394272327423096\n",
      "83: Loss: 0.10032697767019272\n",
      "84: Loss: 0.025336027145385742\n",
      "85: Loss: 0.10755652189254761\n",
      "86: Loss: 0.009040027856826782\n",
      "87: Loss: 0.08231605589389801\n",
      "88: Loss: 0.1224246472120285\n",
      "89: Loss: 0.012816543690860271\n",
      "90: Loss: 0.14929603040218353\n",
      "91: Loss: 0.013577178120613098\n",
      "92: Loss: 0.13276872038841248\n",
      "93: Loss: 0.03446517139673233\n",
      "94: Loss: 0.015544603578746319\n",
      "95: Loss: 0.11579251289367676\n",
      "96: Loss: 0.00493414793163538\n",
      "97: Loss: 0.04336950182914734\n",
      "98: Loss: 0.11137376725673676\n",
      "99: Loss: 0.044334374368190765\n",
      "100: Loss: 0.10313890874385834\n",
      "101: Loss: 0.07183381170034409\n",
      "102: Loss: 0.12156768888235092\n",
      "103: Loss: 0.0454094335436821\n",
      "104: Loss: 0.012594403699040413\n",
      "105: Loss: 0.1159324198961258\n",
      "106: Loss: 0.00993175059556961\n",
      "107: Loss: 0.05336683243513107\n",
      "108: Loss: 0.1370287984609604\n",
      "109: Loss: 0.0043298015370965\n",
      "110: Loss: 0.10524263232946396\n",
      "111: Loss: 0.03223181888461113\n",
      "112: Loss: 0.07902815192937851\n",
      "113: Loss: 0.06054336950182915\n",
      "114: Loss: 0.016732795163989067\n",
      "115: Loss: 0.1653284877538681\n",
      "116: Loss: 0.012037116102874279\n",
      "117: Loss: 0.05182304233312607\n",
      "118: Loss: 0.10707300901412964\n",
      "119: Loss: 0.03940804302692413\n",
      "120: Loss: 0.12669675052165985\n",
      "121: Loss: 0.04630736634135246\n",
      "122: Loss: 0.11963331699371338\n",
      "123: Loss: 0.05093499645590782\n",
      "124: Loss: 0.009086010046303272\n",
      "125: Loss: 0.1486721932888031\n",
      "126: Loss: 0.010329855605959892\n",
      "127: Loss: 0.06404756009578705\n",
      "128: Loss: 0.14068272709846497\n",
      "129: Loss: 0.0253453329205513\n",
      "130: Loss: 0.07707545906305313\n",
      "131: Loss: 0.04066471755504608\n",
      "132: Loss: 0.13667114078998566\n",
      "133: Loss: 0.03727802634239197\n",
      "134: Loss: 0.01934448629617691\n",
      "135: Loss: 0.1463269293308258\n",
      "136: Loss: 0.003330088686197996\n",
      "137: Loss: 0.04214157909154892\n",
      "138: Loss: 0.10477002710103989\n",
      "139: Loss: 0.043653376400470734\n",
      "140: Loss: 0.10316869616508484\n",
      "141: Loss: 0.03067568503320217\n",
      "142: Loss: 0.15082883834838867\n",
      "143: Loss: 0.049733396619558334\n",
      "144: Loss: 0.04346234351396561\n",
      "145: Loss: 0.11715691536664963\n",
      "146: Loss: 0.0189371295273304\n",
      "147: Loss: 0.05416493117809296\n",
      "148: Loss: 0.10705423355102539\n",
      "149: Loss: 0.02109486237168312\n",
      "150: Loss: 0.1044878214597702\n",
      "151: Loss: 0.09161089360713959\n",
      "152: Loss: 0.62099289894104\n",
      "153: Loss: 0.35634443163871765\n",
      "154: Loss: 0.003774121403694153\n",
      "155: Loss: 0.11367256939411163\n",
      "156: Loss: 0.05631343275308609\n",
      "157: Loss: 0.5635786056518555\n",
      "158: Loss: 0.571803867816925\n",
      "159: Loss: 0.03927816450595856\n",
      "160: Loss: 0.11759975552558899\n",
      "161: Loss: 0.032069165259599686\n",
      "162: Loss: 0.09966129064559937\n",
      "163: Loss: 0.046792175620794296\n",
      "164: Loss: 0.03306671977043152\n",
      "165: Loss: 0.09927012771368027\n",
      "166: Loss: 0.008159272372722626\n",
      "167: Loss: 0.07146669924259186\n",
      "168: Loss: 0.11167298257350922\n",
      "169: Loss: 0.038648344576358795\n",
      "170: Loss: 0.08857280015945435\n",
      "171: Loss: 0.0435190312564373\n",
      "172: Loss: 0.12920844554901123\n",
      "173: Loss: 0.03354437276721001\n",
      "174: Loss: 0.017709478735923767\n",
      "175: Loss: 0.12333273142576218\n",
      "176: Loss: 0.014558296650648117\n",
      "177: Loss: 0.05049905925989151\n",
      "178: Loss: 0.09791320562362671\n",
      "179: Loss: 0.0627693384885788\n",
      "180: Loss: 0.09326136112213135\n",
      "181: Loss: 0.05383170396089554\n",
      "182: Loss: 0.10327794402837753\n",
      "183: Loss: 0.04903826862573624\n",
      "184: Loss: 0.013409160077571869\n",
      "185: Loss: 0.1427074670791626\n",
      "186: Loss: 0.008148526772856712\n",
      "187: Loss: 0.057136811316013336\n",
      "188: Loss: 0.12008790671825409\n",
      "189: Loss: 0.03708081692457199\n",
      "190: Loss: 0.11549206078052521\n",
      "191: Loss: 0.023901935666799545\n",
      "192: Loss: 0.0880374014377594\n",
      "193: Loss: 0.03139181062579155\n",
      "194: Loss: 0.05238310247659683\n",
      "195: Loss: 0.13767357170581818\n",
      "196: Loss: 0.006837242282927036\n",
      "197: Loss: 0.08839152753353119\n",
      "198: Loss: 0.12435325235128403\n",
      "199: Loss: 0.015820946544408798\n",
      "200: Loss: 0.07001964747905731\n",
      "201: Loss: 0.07447614520788193\n",
      "202: Loss: 0.09955410659313202\n",
      "203: Loss: 0.05209611356258392\n",
      "204: Loss: 0.04264095425605774\n",
      "205: Loss: 0.15568727254867554\n",
      "206: Loss: 0.014271816238760948\n",
      "207: Loss: 0.06050859019160271\n",
      "208: Loss: 0.10479901731014252\n",
      "209: Loss: 0.02276216819882393\n",
      "210: Loss: 0.09703753888607025\n",
      "211: Loss: 0.05976659804582596\n",
      "212: Loss: 0.11173686385154724\n",
      "213: Loss: 0.056481197476387024\n",
      "214: Loss: 0.038255706429481506\n",
      "215: Loss: 0.14279964566230774\n",
      "216: Loss: 0.008471917361021042\n",
      "217: Loss: 0.05090247839689255\n",
      "218: Loss: 0.12865278124809265\n",
      "219: Loss: 0.035660333931446075\n",
      "220: Loss: 0.11115096509456635\n",
      "221: Loss: 0.02753475308418274\n",
      "222: Loss: 0.09712129831314087\n",
      "223: Loss: 0.04373202845454216\n",
      "224: Loss: 0.0278237946331501\n",
      "225: Loss: 0.09506585448980331\n",
      "226: Loss: 0.01708768494427204\n",
      "227: Loss: 0.052095189690589905\n",
      "228: Loss: 0.12934938073158264\n",
      "229: Loss: 0.015805501490831375\n",
      "230: Loss: 0.07740548253059387\n",
      "231: Loss: 0.0600852370262146\n",
      "232: Loss: 0.076922707259655\n",
      "233: Loss: 0.03418685495853424\n",
      "234: Loss: 0.03221731260418892\n",
      "235: Loss: 0.11727338284254074\n",
      "236: Loss: 0.0054071685299277306\n",
      "237: Loss: 0.04135370999574661\n",
      "238: Loss: 0.11439980566501617\n",
      "239: Loss: 0.029788095504045486\n",
      "240: Loss: 0.08907338231801987\n",
      "241: Loss: 0.07585747539997101\n",
      "242: Loss: 0.10274092853069305\n",
      "243: Loss: 0.05622413381934166\n",
      "244: Loss: 0.0270465686917305\n",
      "245: Loss: 0.09641214460134506\n",
      "246: Loss: 0.0036378949880599976\n",
      "247: Loss: 0.0679270401597023\n",
      "248: Loss: 0.10406376421451569\n",
      "249: Loss: 0.037918731570243835\n",
      "250: Loss: 0.10247218608856201\n",
      "251: Loss: 0.06584145873785019\n",
      "252: Loss: 0.11182942241430283\n",
      "253: Loss: 0.047706492245197296\n",
      "254: Loss: 0.025768999010324478\n",
      "255: Loss: 0.1358567476272583\n",
      "256: Loss: 0.009618481621146202\n",
      "257: Loss: 0.05568883940577507\n",
      "258: Loss: 0.10803455114364624\n",
      "259: Loss: 0.02048429287970066\n",
      "260: Loss: 0.1296798586845398\n",
      "261: Loss: 0.055794037878513336\n",
      "262: Loss: 0.10476946830749512\n",
      "263: Loss: 0.03817278891801834\n",
      "264: Loss: 0.05214095115661621\n",
      "265: Loss: 0.09649999439716339\n",
      "266: Loss: 0.009799592196941376\n",
      "267: Loss: 0.07468390464782715\n",
      "268: Loss: 0.10759305208921432\n",
      "269: Loss: 0.0324944444000721\n",
      "270: Loss: 0.09935328364372253\n",
      "271: Loss: 0.04500766098499298\n",
      "272: Loss: 0.09537351131439209\n",
      "273: Loss: 0.029113750904798508\n",
      "274: Loss: 0.03161286562681198\n",
      "275: Loss: 0.1805010437965393\n",
      "276: Loss: 0.012206517159938812\n",
      "277: Loss: 0.045800160616636276\n",
      "278: Loss: 0.09336809813976288\n",
      "279: Loss: 0.041724979877471924\n",
      "280: Loss: 0.08079874515533447\n",
      "281: Loss: 0.06007324904203415\n",
      "282: Loss: 0.08725830167531967\n",
      "283: Loss: 0.06316651403903961\n",
      "284: Loss: 0.03226533159613609\n",
      "285: Loss: 0.10328780114650726\n",
      "286: Loss: 0.005119772627949715\n",
      "287: Loss: 0.08470724523067474\n",
      "288: Loss: 0.12190511077642441\n",
      "289: Loss: 0.027681034058332443\n",
      "290: Loss: 0.1084543988108635\n",
      "291: Loss: 0.032524239271879196\n",
      "292: Loss: 0.1111002191901207\n",
      "293: Loss: 0.02667153999209404\n",
      "294: Loss: 0.027006741613149643\n",
      "295: Loss: 0.14528179168701172\n",
      "296: Loss: 0.013830495998263359\n",
      "297: Loss: 0.08410706371068954\n",
      "298: Loss: 0.10570885986089706\n",
      "299: Loss: 0.03366536647081375\n",
      "300: Loss: 0.09935466945171356\n",
      "301: Loss: 0.05467812344431877\n",
      "302: Loss: 0.1287207007408142\n",
      "303: Loss: 0.024442389607429504\n",
      "304: Loss: 0.06367863714694977\n",
      "305: Loss: 0.16233095526695251\n",
      "306: Loss: 0.01281602494418621\n",
      "307: Loss: 0.08984319865703583\n",
      "308: Loss: 0.1079101413488388\n",
      "309: Loss: 0.02405693382024765\n",
      "310: Loss: 0.08881531655788422\n",
      "311: Loss: 0.04376959055662155\n",
      "312: Loss: 0.1075013279914856\n",
      "313: Loss: 0.04169745370745659\n",
      "314: Loss: 0.04071341082453728\n",
      "315: Loss: 0.07777003943920135\n",
      "316: Loss: 0.014389647170901299\n",
      "317: Loss: 0.0706082284450531\n",
      "318: Loss: 0.12473408877849579\n",
      "319: Loss: 0.016399405896663666\n",
      "320: Loss: 0.056327491998672485\n",
      "321: Loss: 0.06469197571277618\n",
      "322: Loss: 0.10867078602313995\n",
      "323: Loss: 0.025906242430210114\n",
      "324: Loss: 0.07041668146848679\n",
      "325: Loss: 0.10634651780128479\n",
      "326: Loss: 0.010163286700844765\n",
      "327: Loss: 0.07822196185588837\n",
      "328: Loss: 0.0957193672657013\n",
      "329: Loss: 0.05075247213244438\n",
      "330: Loss: 0.0798606127500534\n",
      "331: Loss: 0.06756071746349335\n",
      "332: Loss: 0.11184041947126389\n",
      "333: Loss: 0.03032606840133667\n",
      "334: Loss: 0.04370688647031784\n",
      "335: Loss: 0.09193017333745956\n",
      "336: Loss: 0.015825342386960983\n",
      "337: Loss: 0.0419677197933197\n",
      "338: Loss: 0.09871247410774231\n",
      "339: Loss: 0.044315315783023834\n",
      "340: Loss: 0.09066348522901535\n",
      "341: Loss: 0.07246308773756027\n",
      "342: Loss: 0.10977745056152344\n",
      "343: Loss: 0.028461866080760956\n",
      "344: Loss: 0.056338563561439514\n",
      "345: Loss: 0.06531856954097748\n",
      "346: Loss: 0.019443050026893616\n",
      "347: Loss: 0.05632263049483299\n",
      "348: Loss: 0.09432268142700195\n",
      "349: Loss: 0.03986705094575882\n",
      "350: Loss: 0.12866199016571045\n",
      "351: Loss: 4.022398948669434\n",
      "352: Loss: 3.9487223625183105\n",
      "353: Loss: 3.529778242111206\n",
      "354: Loss: 4.350061416625977\n",
      "355: Loss: 4.344988822937012\n",
      "356: Loss: 3.979231834411621\n",
      "357: Loss: 3.5616555213928223\n",
      "358: Loss: 4.414971351623535\n",
      "359: Loss: 0.041954170912504196\n",
      "360: Loss: 0.08022468537092209\n",
      "361: Loss: 0.057672854512929916\n",
      "362: Loss: 0.0964740663766861\n",
      "363: Loss: 0.023133929818868637\n",
      "364: Loss: 0.04072052240371704\n",
      "365: Loss: 0.10387247800827026\n",
      "366: Loss: 0.015283094719052315\n",
      "367: Loss: 0.07205080986022949\n",
      "368: Loss: 0.08536031097173691\n",
      "369: Loss: 0.035501133650541306\n",
      "370: Loss: 0.11032731831073761\n",
      "371: Loss: 0.05498890578746796\n",
      "372: Loss: 0.09045016765594482\n",
      "373: Loss: 0.04207286238670349\n",
      "374: Loss: 0.05381765589118004\n",
      "375: Loss: 0.13594841957092285\n",
      "376: Loss: 0.018220391124486923\n",
      "377: Loss: 0.06104951351881027\n",
      "378: Loss: 0.08798342198133469\n",
      "379: Loss: 0.06328074634075165\n",
      "380: Loss: 0.08680687844753265\n",
      "381: Loss: 0.06823552399873734\n",
      "382: Loss: 0.0899755209684372\n",
      "383: Loss: 0.03260514512658119\n",
      "384: Loss: 0.04023417830467224\n",
      "385: Loss: 0.08882097899913788\n",
      "386: Loss: 0.023258313536643982\n",
      "387: Loss: 0.08702036738395691\n",
      "388: Loss: 0.1295837163925171\n",
      "389: Loss: 0.04138890281319618\n",
      "390: Loss: 0.05472820997238159\n",
      "391: Loss: 0.0676657035946846\n",
      "392: Loss: 0.1043199896812439\n",
      "393: Loss: 0.0361214205622673\n",
      "394: Loss: 0.03932779282331467\n",
      "395: Loss: 0.07456831634044647\n",
      "396: Loss: 0.01203174889087677\n",
      "397: Loss: 0.07917296886444092\n",
      "398: Loss: 0.11025115102529526\n",
      "399: Loss: 0.03887992352247238\n",
      "400: Loss: 0.06322590261697769\n",
      "401: Loss: 0.06006428226828575\n",
      "402: Loss: 0.0811876654624939\n",
      "403: Loss: 0.046759676188230515\n",
      "404: Loss: 0.07818824797868729\n",
      "405: Loss: 0.08390866219997406\n",
      "406: Loss: 0.018021926283836365\n",
      "407: Loss: 0.058925531804561615\n",
      "408: Loss: 0.11132638156414032\n",
      "409: Loss: 0.035231515765190125\n",
      "410: Loss: 0.08074238151311874\n",
      "411: Loss: 0.09475010633468628\n",
      "412: Loss: 0.09037800878286362\n",
      "413: Loss: 0.02341650053858757\n",
      "414: Loss: 0.05079413577914238\n",
      "415: Loss: 0.0953236073255539\n",
      "416: Loss: 0.01277124136686325\n",
      "417: Loss: 0.0792374387383461\n",
      "418: Loss: 0.1227322444319725\n",
      "419: Loss: 0.044666022062301636\n",
      "420: Loss: 0.09553449600934982\n",
      "421: Loss: 0.06480073928833008\n",
      "422: Loss: 0.10546089708805084\n",
      "423: Loss: 0.02724413201212883\n",
      "424: Loss: 0.07762448489665985\n",
      "425: Loss: 0.08324864506721497\n",
      "426: Loss: 0.023677345365285873\n",
      "427: Loss: 0.06764963269233704\n",
      "428: Loss: 0.11274945735931396\n",
      "429: Loss: 0.053806792944669724\n",
      "430: Loss: 0.06676055490970612\n",
      "431: Loss: 0.05270151048898697\n",
      "432: Loss: 0.08806996792554855\n",
      "433: Loss: 0.027025684714317322\n",
      "434: Loss: 0.10211174935102463\n",
      "435: Loss: 0.10846685618162155\n",
      "436: Loss: 0.015120367519557476\n",
      "437: Loss: 0.08293157815933228\n",
      "438: Loss: 0.11037903279066086\n",
      "439: Loss: 0.04918670281767845\n",
      "440: Loss: 0.08522403240203857\n",
      "441: Loss: 0.08546607196331024\n",
      "442: Loss: 0.09290337562561035\n",
      "443: Loss: 0.03312539681792259\n",
      "444: Loss: 0.05266934260725975\n",
      "445: Loss: 0.09887578338384628\n",
      "446: Loss: 0.0164672639220953\n",
      "447: Loss: 0.08366995304822922\n",
      "448: Loss: 0.09961720556020737\n",
      "449: Loss: 0.060973428189754486\n",
      "450: Loss: 0.05461006611585617\n",
      "451: Loss: 0.09552981704473495\n",
      "452: Loss: 0.12316002696752548\n",
      "453: Loss: 0.018186025321483612\n",
      "454: Loss: 0.05898185074329376\n",
      "455: Loss: 0.06536947190761566\n",
      "456: Loss: 0.019197069108486176\n",
      "457: Loss: 0.08130154758691788\n",
      "458: Loss: 0.0884576141834259\n",
      "459: Loss: 0.03758152946829796\n",
      "460: Loss: 0.08041970431804657\n",
      "461: Loss: 0.09337713569402695\n",
      "462: Loss: 0.07247577607631683\n",
      "463: Loss: 0.02815786562860012\n",
      "464: Loss: 0.07932904362678528\n",
      "465: Loss: 0.07659400999546051\n",
      "466: Loss: 0.012564733624458313\n",
      "467: Loss: 0.06848542392253876\n",
      "468: Loss: 0.09157048165798187\n",
      "469: Loss: 0.08128852397203445\n",
      "470: Loss: 0.03927866742014885\n",
      "471: Loss: 0.10837342590093613\n",
      "472: Loss: 0.11711841821670532\n",
      "473: Loss: 0.0308158528059721\n",
      "474: Loss: 0.0781586766242981\n",
      "475: Loss: 0.0658746287226677\n",
      "476: Loss: 0.037374258041381836\n",
      "477: Loss: 0.10680856555700302\n",
      "478: Loss: 0.06117492914199829\n",
      "479: Loss: 0.06867827475070953\n",
      "480: Loss: 0.10097938776016235\n",
      "481: Loss: 0.07757023721933365\n",
      "482: Loss: 0.08160742372274399\n",
      "483: Loss: 0.020821860060095787\n",
      "484: Loss: 0.08963224291801453\n",
      "485: Loss: 0.06558813154697418\n",
      "486: Loss: 0.02895931527018547\n",
      "487: Loss: 0.12873606383800507\n",
      "488: Loss: 0.0961163341999054\n",
      "489: Loss: 0.0491153746843338\n",
      "490: Loss: 0.042429737746715546\n",
      "491: Loss: 0.08543019741773605\n",
      "492: Loss: 0.11523899435997009\n",
      "493: Loss: 0.03515547141432762\n",
      "494: Loss: 0.11006656289100647\n",
      "495: Loss: 0.0761612206697464\n",
      "496: Loss: 0.030409131199121475\n",
      "497: Loss: 0.07914762198925018\n",
      "498: Loss: 0.09159356355667114\n",
      "499: Loss: 0.029292358085513115\n",
      "500: Loss: 0.05434342473745346\n",
      "501: Loss: 0.11384187638759613\n",
      "502: Loss: 0.07385004311800003\n",
      "503: Loss: 0.01968187838792801\n",
      "504: Loss: 0.040078554302453995\n",
      "505: Loss: 0.0756535679101944\n",
      "506: Loss: 0.023872485384345055\n",
      "507: Loss: 0.08282317221164703\n",
      "508: Loss: 0.0598793551325798\n",
      "509: Loss: 0.0735199972987175\n",
      "510: Loss: 0.06069592386484146\n",
      "511: Loss: 0.07388083636760712\n",
      "512: Loss: 0.0954943597316742\n",
      "513: Loss: 0.035198189318180084\n",
      "514: Loss: 0.07179224491119385\n",
      "515: Loss: 0.09221049398183823\n",
      "516: Loss: 0.022512083873152733\n",
      "517: Loss: 0.08579322695732117\n",
      "518: Loss: 0.08135876059532166\n",
      "519: Loss: 0.06300654262304306\n",
      "520: Loss: 0.0880197137594223\n",
      "521: Loss: 0.08891334384679794\n",
      "522: Loss: 0.08123903721570969\n",
      "523: Loss: 0.02278212457895279\n",
      "524: Loss: 0.07078193128108978\n",
      "525: Loss: 0.06766629219055176\n",
      "526: Loss: 0.02965037152171135\n",
      "527: Loss: 0.08270925283432007\n",
      "528: Loss: 0.08992992341518402\n",
      "529: Loss: 0.058216895908117294\n",
      "530: Loss: 0.04733501002192497\n",
      "531: Loss: 0.10384508967399597\n",
      "532: Loss: 0.09244534373283386\n",
      "533: Loss: 0.015624046325683594\n",
      "534: Loss: 0.06191809102892876\n",
      "535: Loss: 0.08175279200077057\n",
      "536: Loss: 0.025502130389213562\n",
      "537: Loss: 0.0897158533334732\n",
      "538: Loss: 0.14359267055988312\n",
      "539: Loss: 0.06832557916641235\n",
      "540: Loss: 0.04769434779882431\n",
      "541: Loss: 0.08291715383529663\n",
      "542: Loss: 0.056293025612831116\n",
      "543: Loss: 0.019309088587760925\n",
      "544: Loss: 0.05735693499445915\n",
      "545: Loss: 0.06668399274349213\n",
      "546: Loss: 0.022040046751499176\n",
      "547: Loss: 0.09145800769329071\n",
      "548: Loss: 0.06559156626462936\n",
      "549: Loss: 0.06845594942569733\n",
      "550: Loss: 0.06710925698280334\n",
      "551: Loss: 0.08518179506063461\n",
      "552: Loss: 0.06744024902582169\n",
      "553: Loss: 0.02108089253306389\n",
      "554: Loss: 0.10294374078512192\n",
      "555: Loss: 0.0656387209892273\n",
      "556: Loss: 0.036552563309669495\n",
      "557: Loss: 0.09136965125799179\n",
      "558: Loss: 0.09159611165523529\n",
      "559: Loss: 0.1023828536272049\n",
      "560: Loss: 0.04588449001312256\n",
      "561: Loss: 0.10647766292095184\n",
      "562: Loss: 0.08368917554616928\n",
      "563: Loss: 0.01610712707042694\n",
      "564: Loss: 0.10185796022415161\n",
      "565: Loss: 0.07573196291923523\n",
      "566: Loss: 0.01829511672258377\n",
      "567: Loss: 0.07156366109848022\n",
      "568: Loss: 0.10468443483114243\n",
      "569: Loss: 0.106329046189785\n",
      "570: Loss: 0.05977373570203781\n",
      "571: Loss: 0.0926012247800827\n",
      "572: Loss: 0.0903162881731987\n",
      "573: Loss: 0.015336917713284492\n",
      "574: Loss: 0.06165902316570282\n",
      "575: Loss: 0.05747602507472038\n",
      "576: Loss: 0.0331680029630661\n",
      "577: Loss: 0.0797421857714653\n",
      "578: Loss: 0.07818152755498886\n",
      "579: Loss: 0.06296141445636749\n",
      "580: Loss: 0.035505667328834534\n",
      "581: Loss: 0.10347684472799301\n",
      "582: Loss: 0.0919458270072937\n",
      "583: Loss: 0.018131405115127563\n",
      "584: Loss: 0.09618198871612549\n",
      "585: Loss: 0.10885600745677948\n",
      "586: Loss: 0.030653325840830803\n",
      "587: Loss: 0.09073936194181442\n",
      "588: Loss: 0.06483113765716553\n",
      "589: Loss: 0.09109421074390411\n",
      "590: Loss: 0.03771623969078064\n",
      "591: Loss: 0.1022600382566452\n",
      "592: Loss: 0.06873133778572083\n",
      "593: Loss: 0.011531653814017773\n",
      "594: Loss: 0.10558181256055832\n",
      "595: Loss: 0.0632559210062027\n",
      "596: Loss: 0.05594634264707565\n",
      "597: Loss: 0.09129665791988373\n",
      "598: Loss: 0.09787406027317047\n",
      "599: Loss: 0.0773254856467247\n",
      "600: Loss: 0.038440603762865067\n",
      "601: Loss: 0.11258172988891602\n",
      "602: Loss: 0.07860896736383438\n",
      "603: Loss: 0.020202236250042915\n",
      "604: Loss: 0.09061729907989502\n",
      "605: Loss: 0.07306491583585739\n",
      "606: Loss: 0.021982867270708084\n",
      "607: Loss: 0.08651082962751389\n",
      "608: Loss: 0.07375755906105042\n",
      "609: Loss: 0.06862151622772217\n",
      "610: Loss: 0.044349636882543564\n",
      "611: Loss: 0.11132320016622543\n",
      "612: Loss: 0.06911826878786087\n",
      "613: Loss: 0.022102169692516327\n",
      "614: Loss: 0.08304878324270248\n",
      "615: Loss: 0.05283694714307785\n",
      "616: Loss: 0.0423898808658123\n",
      "617: Loss: 0.10082845389842987\n",
      "618: Loss: 0.0819678008556366\n",
      "619: Loss: 0.06918373703956604\n",
      "620: Loss: 0.02312234789133072\n",
      "621: Loss: 0.12197871506214142\n",
      "622: Loss: 0.07674314826726913\n",
      "623: Loss: 0.03011387214064598\n",
      "624: Loss: 0.08558551967144012\n",
      "625: Loss: 0.04294159635901451\n",
      "626: Loss: 0.04954291135072708\n",
      "627: Loss: 0.14175456762313843\n",
      "628: Loss: 0.08427564799785614\n",
      "629: Loss: 0.07385816425085068\n",
      "630: Loss: 0.05185142904520035\n",
      "631: Loss: 0.13549762964248657\n",
      "632: Loss: 0.08887693285942078\n",
      "633: Loss: 0.01361037790775299\n",
      "634: Loss: 0.16341936588287354\n",
      "635: Loss: 0.048237305134534836\n",
      "636: Loss: 0.03622731193900108\n",
      "637: Loss: 0.09940595924854279\n",
      "638: Loss: 0.05784373730421066\n",
      "639: Loss: 0.059338461607694626\n",
      "640: Loss: 0.03569268807768822\n",
      "641: Loss: 0.10564934462308884\n",
      "642: Loss: 0.06719138473272324\n",
      "643: Loss: 0.017954718321561813\n",
      "644: Loss: 0.10355257242918015\n",
      "645: Loss: 0.05009661614894867\n",
      "646: Loss: 0.03293610364198685\n",
      "647: Loss: 0.08981795608997345\n",
      "648: Loss: 0.06103336811065674\n",
      "649: Loss: 0.06785927712917328\n",
      "650: Loss: 0.0596376471221447\n",
      "651: Loss: 0.1015106663107872\n",
      "652: Loss: 0.06590383499860764\n",
      "653: Loss: 0.0212344191968441\n",
      "654: Loss: 0.09343194961547852\n",
      "655: Loss: 0.029642537236213684\n",
      "656: Loss: 0.030101045966148376\n",
      "657: Loss: 0.10695459693670273\n",
      "658: Loss: 0.0728522539138794\n",
      "659: Loss: 0.07156479358673096\n",
      "660: Loss: 0.032015345990657806\n",
      "661: Loss: 0.11257694661617279\n",
      "662: Loss: 0.069563128054142\n",
      "663: Loss: 0.03272204101085663\n",
      "664: Loss: 0.12810048460960388\n",
      "665: Loss: 0.044954560697078705\n",
      "666: Loss: 0.029093995690345764\n",
      "667: Loss: 0.1245490312576294\n",
      "668: Loss: 0.1025892123579979\n",
      "669: Loss: 0.10064461082220078\n",
      "670: Loss: 0.059675730764865875\n",
      "671: Loss: 0.11335652321577072\n",
      "672: Loss: 0.057452231645584106\n",
      "673: Loss: 0.019561029970645905\n",
      "674: Loss: 0.1249321699142456\n",
      "675: Loss: 0.03214673325419426\n",
      "676: Loss: 0.03311832994222641\n",
      "677: Loss: 0.08780833333730698\n",
      "678: Loss: 0.049947455525398254\n",
      "679: Loss: 0.12893258035182953\n",
      "680: Loss: 0.1641232818365097\n",
      "681: Loss: 0.0968528538942337\n",
      "682: Loss: 0.33454930782318115\n",
      "683: Loss: 0.5964236259460449\n",
      "684: Loss: 0.11392485350370407\n",
      "685: Loss: 0.0807693749666214\n",
      "686: Loss: 0.8743830919265747\n",
      "687: Loss: 0.10670261830091476\n",
      "688: Loss: 0.029494991526007652\n",
      "689: Loss: 0.8440107703208923\n",
      "690: Loss: 0.31478583812713623\n",
      "691: Loss: 0.12504281103610992\n",
      "692: Loss: 0.2574383616447449\n",
      "693: Loss: 0.645331621170044\n",
      "694: Loss: 0.08766244351863861\n",
      "695: Loss: 0.08219867944717407\n",
      "696: Loss: 0.9287598133087158\n",
      "697: Loss: 0.08206372708082199\n",
      "698: Loss: 0.018963834270834923\n",
      "699: Loss: 0.8325652480125427\n",
      "700: Loss: 0.352251261472702\n",
      "701: Loss: 0.1433136761188507\n",
      "702: Loss: 0.2726730704307556\n",
      "703: Loss: 0.19611166417598724\n",
      "704: Loss: 0.13111639022827148\n",
      "705: Loss: 0.04881065711379051\n",
      "706: Loss: 0.04334377497434616\n",
      "707: Loss: 0.11651354283094406\n",
      "708: Loss: 0.06932375580072403\n",
      "709: Loss: 0.06634600460529327\n",
      "710: Loss: 0.04124302417039871\n",
      "711: Loss: 0.08932451903820038\n",
      "712: Loss: 0.09071149677038193\n",
      "713: Loss: 0.0065530044957995415\n",
      "714: Loss: 0.11362050473690033\n",
      "715: Loss: 0.0446348711848259\n",
      "716: Loss: 0.04437549039721489\n",
      "717: Loss: 0.10851418972015381\n",
      "718: Loss: 0.04650355130434036\n",
      "719: Loss: 0.10024039447307587\n",
      "720: Loss: 0.020662451162934303\n",
      "721: Loss: 0.11067014187574387\n",
      "722: Loss: 0.069584421813488\n",
      "723: Loss: 0.02330510877072811\n",
      "724: Loss: 0.11995097249746323\n",
      "725: Loss: 0.04386879503726959\n",
      "726: Loss: 0.030186524614691734\n",
      "727: Loss: 0.08279360830783844\n",
      "728: Loss: 0.04691670462489128\n",
      "729: Loss: 0.0986199676990509\n",
      "730: Loss: 0.009633716195821762\n",
      "731: Loss: 0.10953843593597412\n",
      "732: Loss: 0.07605570554733276\n",
      "733: Loss: 0.0057672737166285515\n",
      "734: Loss: 0.09722819179296494\n",
      "735: Loss: 0.05732911825180054\n",
      "736: Loss: 0.038858797401189804\n",
      "737: Loss: 0.09678725898265839\n",
      "738: Loss: 0.03841165453195572\n",
      "739: Loss: 0.10190783441066742\n",
      "740: Loss: 0.019539913162589073\n",
      "741: Loss: 0.1217571347951889\n",
      "742: Loss: 0.07330726087093353\n",
      "743: Loss: 0.00707579217851162\n",
      "744: Loss: 0.09857821464538574\n",
      "745: Loss: 0.037234582006931305\n",
      "746: Loss: 0.03192255273461342\n",
      "747: Loss: 0.09284840524196625\n",
      "748: Loss: 0.06219475716352463\n",
      "749: Loss: 0.12142336368560791\n",
      "750: Loss: 0.0489739291369915\n",
      "751: Loss: 0.09672889113426208\n",
      "752: Loss: 0.05821557343006134\n",
      "753: Loss: 0.012122418731451035\n",
      "754: Loss: 0.08461034297943115\n",
      "755: Loss: 0.07973755896091461\n",
      "756: Loss: 0.04921525716781616\n",
      "757: Loss: 0.06761488318443298\n",
      "758: Loss: 0.08389030396938324\n",
      "759: Loss: 0.10578899830579758\n",
      "760: Loss: 0.0251174233853817\n",
      "761: Loss: 0.08378751575946808\n",
      "762: Loss: 0.06778275966644287\n",
      "763: Loss: 0.012956170365214348\n",
      "764: Loss: 0.11886575073003769\n",
      "765: Loss: 0.019762184470891953\n",
      "766: Loss: 0.04109208285808563\n",
      "767: Loss: 0.11795204132795334\n",
      "768: Loss: 0.03644919767975807\n",
      "769: Loss: 0.10605306923389435\n",
      "770: Loss: 0.028281955048441887\n",
      "771: Loss: 0.12030956149101257\n",
      "772: Loss: 0.0893959030508995\n",
      "773: Loss: 0.016224276274442673\n",
      "774: Loss: 0.09926068782806396\n",
      "775: Loss: 0.023518159985542297\n",
      "776: Loss: 0.04166552051901817\n",
      "777: Loss: 0.12149136513471603\n",
      "778: Loss: 0.038890402764081955\n",
      "779: Loss: 0.10055606067180634\n",
      "780: Loss: 0.02352362498641014\n",
      "781: Loss: 0.1347951591014862\n",
      "782: Loss: 0.048035603016614914\n",
      "783: Loss: 0.003153648693114519\n",
      "784: Loss: 0.17088094353675842\n",
      "785: Loss: 0.05305607244372368\n",
      "786: Loss: 0.040737345814704895\n",
      "787: Loss: 0.08708664029836655\n",
      "788: Loss: 0.044384803622961044\n",
      "789: Loss: 0.09737749397754669\n",
      "790: Loss: 0.03629603609442711\n",
      "791: Loss: 0.10489251464605331\n",
      "792: Loss: 0.04837976023554802\n",
      "793: Loss: 0.009459368884563446\n",
      "794: Loss: 0.13198187947273254\n",
      "795: Loss: 0.04682724177837372\n",
      "796: Loss: 0.04082436114549637\n",
      "797: Loss: 0.12660770118236542\n",
      "798: Loss: 0.041914574801921844\n",
      "799: Loss: 0.11174094676971436\n",
      "800: Loss: 0.022879276424646378\n",
      "801: Loss: 0.13049091398715973\n",
      "802: Loss: 0.07962650060653687\n",
      "803: Loss: 0.014380729757249355\n",
      "804: Loss: 0.129885733127594\n",
      "805: Loss: 0.011277209036052227\n",
      "806: Loss: 0.05297962948679924\n",
      "807: Loss: 0.144467294216156\n",
      "808: Loss: 0.0580703541636467\n",
      "809: Loss: 0.09052188694477081\n",
      "810: Loss: 0.06377051770687103\n",
      "811: Loss: 0.14399361610412598\n",
      "812: Loss: 0.0537073127925396\n",
      "813: Loss: 0.005376926623284817\n",
      "814: Loss: 0.1340252012014389\n",
      "815: Loss: 0.02068333700299263\n",
      "816: Loss: 0.036153748631477356\n",
      "817: Loss: 0.13714957237243652\n",
      "818: Loss: 0.039956193417310715\n",
      "819: Loss: 0.08794718235731125\n",
      "820: Loss: 0.04121812805533409\n",
      "821: Loss: 0.11517433822154999\n",
      "822: Loss: 0.06917969882488251\n",
      "823: Loss: 0.006483386270701885\n",
      "824: Loss: 0.15175805985927582\n",
      "825: Loss: 0.02968493476510048\n",
      "826: Loss: 0.04143190383911133\n",
      "827: Loss: 0.1305975764989853\n",
      "828: Loss: 0.04470431059598923\n",
      "829: Loss: 0.10289754718542099\n",
      "830: Loss: 0.01634354703128338\n",
      "831: Loss: 0.1451365053653717\n",
      "832: Loss: 0.09044033288955688\n",
      "833: Loss: 0.013150474056601524\n",
      "834: Loss: 0.11707237362861633\n",
      "835: Loss: 0.03944604843854904\n",
      "836: Loss: 0.035060666501522064\n",
      "837: Loss: 0.13304376602172852\n",
      "838: Loss: 0.036732740700244904\n",
      "839: Loss: 0.07999520003795624\n",
      "840: Loss: 0.019165856763720512\n",
      "841: Loss: 0.1032547801733017\n",
      "842: Loss: 0.05242964252829552\n",
      "843: Loss: 0.011811396107077599\n",
      "844: Loss: 0.12026134133338928\n",
      "845: Loss: 0.0037224977277219296\n",
      "846: Loss: 0.02963070571422577\n",
      "847: Loss: 0.11581671237945557\n",
      "848: Loss: 0.033522091805934906\n",
      "849: Loss: 0.10392254590988159\n",
      "850: Loss: 0.011497227475047112\n",
      "851: Loss: 0.12982173264026642\n",
      "852: Loss: 0.06569385528564453\n",
      "853: Loss: 0.007672054227441549\n",
      "854: Loss: 0.1392362415790558\n",
      "855: Loss: 0.021144768223166466\n",
      "856: Loss: 0.048107024282217026\n",
      "857: Loss: 0.11763520538806915\n",
      "858: Loss: 0.0321870893239975\n",
      "859: Loss: 0.11012086272239685\n",
      "860: Loss: 0.020000305026769638\n",
      "861: Loss: 0.08548810333013535\n",
      "862: Loss: 0.05362708866596222\n",
      "863: Loss: 0.013777893036603928\n",
      "864: Loss: 0.1383221447467804\n",
      "865: Loss: 0.006884714122861624\n",
      "866: Loss: 0.057872604578733444\n",
      "867: Loss: 0.11216165125370026\n",
      "868: Loss: 0.0449506938457489\n",
      "869: Loss: 0.07522433996200562\n",
      "870: Loss: 0.02792191132903099\n",
      "871: Loss: 0.12400486320257187\n",
      "872: Loss: 0.08806364238262177\n",
      "873: Loss: 0.010049727745354176\n",
      "874: Loss: 0.08700987696647644\n",
      "875: Loss: 0.013549963012337685\n",
      "876: Loss: 0.046729132533073425\n",
      "877: Loss: 0.11718052625656128\n",
      "878: Loss: 0.05820455402135849\n",
      "879: Loss: 0.11238503456115723\n",
      "880: Loss: 0.035919517278671265\n",
      "881: Loss: 0.135856494307518\n",
      "882: Loss: 0.07007542997598648\n",
      "883: Loss: 0.016174202784895897\n",
      "884: Loss: 0.1336904913187027\n",
      "885: Loss: 0.03073713928461075\n",
      "886: Loss: 0.054966188967227936\n",
      "887: Loss: 0.15707288682460785\n",
      "888: Loss: 0.017023906111717224\n",
      "889: Loss: 0.08698859810829163\n",
      "890: Loss: 0.006650174967944622\n",
      "891: Loss: 0.12650743126869202\n",
      "892: Loss: 0.05471113324165344\n",
      "893: Loss: 0.005342930555343628\n",
      "894: Loss: 0.14264646172523499\n",
      "895: Loss: 0.019302668049931526\n",
      "896: Loss: 0.04270832985639572\n",
      "897: Loss: 0.1104879379272461\n",
      "898: Loss: 0.0348413810133934\n",
      "899: Loss: 0.11315491050481796\n",
      "900: Loss: 0.03243429958820343\n",
      "901: Loss: 0.11107924580574036\n",
      "902: Loss: 0.053392376750707626\n",
      "903: Loss: 0.008065213449299335\n",
      "904: Loss: 0.1559598594903946\n",
      "905: Loss: 0.017910093069076538\n",
      "906: Loss: 0.06665105372667313\n",
      "907: Loss: 0.11879607290029526\n",
      "908: Loss: 0.03435559570789337\n",
      "909: Loss: 0.0909411832690239\n",
      "910: Loss: 0.028820250183343887\n",
      "911: Loss: 0.12866006791591644\n",
      "912: Loss: 0.06198381632566452\n",
      "913: Loss: 0.024680597707629204\n",
      "914: Loss: 0.14750587940216064\n",
      "915: Loss: 0.02964487299323082\n",
      "916: Loss: 0.06553678214550018\n",
      "917: Loss: 0.11232777684926987\n",
      "918: Loss: 0.02762673608958721\n",
      "919: Loss: 0.09542863816022873\n",
      "920: Loss: 0.004317441023886204\n",
      "921: Loss: 0.13619624078273773\n",
      "922: Loss: 0.03799913823604584\n",
      "923: Loss: 0.010341111570596695\n",
      "924: Loss: 0.13508012890815735\n",
      "925: Loss: 0.011600143276154995\n",
      "926: Loss: 0.039521023631095886\n",
      "927: Loss: 0.13817250728607178\n",
      "928: Loss: 0.0279521644115448\n",
      "929: Loss: 0.09882684797048569\n",
      "930: Loss: 0.029802311211824417\n",
      "931: Loss: 0.11734670400619507\n",
      "932: Loss: 0.03975551202893257\n",
      "933: Loss: 0.012202776968479156\n",
      "934: Loss: 0.14348584413528442\n",
      "935: Loss: 0.014763991348445415\n",
      "936: Loss: 0.057421714067459106\n",
      "937: Loss: 0.13086900115013123\n",
      "938: Loss: 0.015512116253376007\n",
      "939: Loss: 0.10817857831716537\n",
      "940: Loss: 0.026049938052892685\n",
      "941: Loss: 0.10866552591323853\n",
      "942: Loss: 0.043314870446920395\n",
      "943: Loss: 0.01346970722079277\n",
      "944: Loss: 0.1584741473197937\n",
      "945: Loss: 0.019589290022850037\n",
      "946: Loss: 0.05053500458598137\n",
      "947: Loss: 0.1402461677789688\n",
      "948: Loss: 0.02419126033782959\n",
      "949: Loss: 0.10586806386709213\n",
      "950: Loss: 0.027429688721895218\n",
      "951: Loss: 0.13622717559337616\n",
      "952: Loss: 0.05975726991891861\n",
      "953: Loss: 0.01727771945297718\n",
      "954: Loss: 0.17596381902694702\n",
      "955: Loss: 0.01285835262387991\n",
      "956: Loss: 0.053238868713378906\n",
      "957: Loss: 0.1444462388753891\n",
      "958: Loss: 0.02531752549111843\n",
      "959: Loss: 0.09459640830755234\n",
      "960: Loss: 0.03426569327712059\n",
      "961: Loss: 0.10469674319028854\n",
      "962: Loss: 0.05608570575714111\n",
      "963: Loss: 0.014390787109732628\n",
      "964: Loss: 0.12527760863304138\n",
      "965: Loss: 0.029779814183712006\n",
      "966: Loss: 0.043682679533958435\n",
      "967: Loss: 0.1187838539481163\n",
      "968: Loss: 0.04225759208202362\n",
      "969: Loss: 0.14160750806331635\n",
      "970: Loss: 0.035883378237485886\n",
      "971: Loss: 0.11373847723007202\n",
      "972: Loss: 0.03649711236357689\n",
      "973: Loss: 0.0075035300105810165\n",
      "974: Loss: 0.11073406785726547\n",
      "975: Loss: 0.003892898093909025\n",
      "976: Loss: 0.04713523015379906\n",
      "977: Loss: 0.12902073562145233\n",
      "978: Loss: 0.020414844155311584\n",
      "979: Loss: 0.09864389896392822\n",
      "980: Loss: 0.041818492114543915\n",
      "981: Loss: 0.13212046027183533\n",
      "982: Loss: 0.05940984934568405\n",
      "983: Loss: 0.006392035633325577\n",
      "984: Loss: 0.13324211537837982\n",
      "985: Loss: 0.019373062998056412\n",
      "986: Loss: 0.04987005516886711\n",
      "987: Loss: 0.09359234571456909\n",
      "988: Loss: 0.035316258668899536\n",
      "989: Loss: 0.09655668586492538\n",
      "990: Loss: 0.048939209431409836\n",
      "991: Loss: 0.12396763265132904\n",
      "992: Loss: 0.058811038732528687\n",
      "993: Loss: 0.018151072785258293\n",
      "994: Loss: 0.2126532793045044\n",
      "995: Loss: 0.01758497580885887\n",
      "996: Loss: 0.05006971210241318\n",
      "997: Loss: 0.12338404357433319\n",
      "998: Loss: 0.05552966147661209\n",
      "999: Loss: 0.14325180649757385\n"
     ]
    }
   ],
   "source": [
    "# Now you can use the trained model to make predictions on dataLoaderA\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(dataLoaderA):\n",
    "        input_data = data.unsqueeze(1).to(args.device)\n",
    "        # Initialize hidden and cell states\n",
    "        batch_size = input_data.size(0)\n",
    "        hidden = torch.zeros((args.layers * 1, batch_size, args.hiddenSize)).to(args.device)\n",
    "        cell = torch.zeros((args.layers * 1, batch_size, args.hiddenSize)).to(args.device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input_data, (hidden, cell))\n",
    "        loss = model.criterion(output, input_data)\n",
    "        print(f\"{i}: Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
